{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "#!pip install regex\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from logparser import Spell, Drain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get [log key, delta time] as input for deeplog\n",
    "input_dir  = '../Datasets/hdfs/'\n",
    "output_dir = '../output/hdfs/'  # The output directory of parsing results\n",
    "log_file   = \"HDFS.log\"  # The input log file name\n",
    "\n",
    "log_structured_file = output_dir + log_file + \"_structured.csv\"\n",
    "log_templates_file = output_dir + log_file + \"_templates.csv\"\n",
    "log_sequence_file = output_dir + \"hdfs_sequence.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(input_dir, output_dir, log_file, log_format, type='drain'):\n",
    "    if type == 'spell':\n",
    "        tau        = 0.5  # Message type threshold (default: 0.5)\n",
    "        regex      = [\n",
    "            \"(/[-\\w]+)+\", #replace file path with *\n",
    "            \"(?<=blk_)[-\\d]+\" #replace block_id with *\n",
    "\n",
    "        ]  # Regular expression list for optional preprocessing (default: [])\n",
    "\n",
    "        parser = Spell.LogParser(indir=input_dir, outdir=output_dir, log_format=log_format, tau=tau, rex=regex, keep_para=False)\n",
    "        parser.parse(log_file)\n",
    "\n",
    "    elif type == 'drain':\n",
    "        regex = [\n",
    "            r\"(?<=blk_)[-\\d]+\", # block_id\n",
    "            r'\\d+\\.\\d+\\.\\d+\\.\\d+',  # IP\n",
    "            r\"(/[-\\w]+)+\",  # file path\n",
    "            #r'(?<=[^A-Za-z0-9])(\\-?\\+?\\d+)(?=[^A-Za-z0-9])|[0-9]+$',  # Numbers\n",
    "        ]\n",
    "        # the hyper parameter is set according to http://jmzhu.logpai.com/pub/pjhe_icws2017.pdf\n",
    "        st = 0.5  # Similarity threshold\n",
    "        depth = 5  # Depth of all leaf nodes\n",
    "\n",
    "\n",
    "        parser = Drain.LogParser(log_format, indir=input_dir, outdir=output_dir, depth=depth, st=st, rex=regex, keep_para=False)\n",
    "        parser.parse(log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping():\n",
    "    log_temp = pd.read_csv(log_templates_file)\n",
    "    log_temp.sort_values(by = [\"Occurrences\"], ascending=False, inplace=True)\n",
    "    log_temp_dict = {event: idx+1 for idx , event in enumerate(list(log_temp[\"EventId\"])) }\n",
    "    print(log_temp_dict)\n",
    "    with open (output_dir + \"hdfs_log_templates.json\", \"w\") as f:\n",
    "        json.dump(log_temp_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdfs_sampling(log_file, window='session'):\n",
    "    assert window == 'session', \"Only window=session is supported for HDFS dataset.\"\n",
    "    print(\"Loading\", log_file)\n",
    "    df = pd.read_csv(log_file, engine='c',\n",
    "            na_filter=False, memory_map=True, dtype={'Date':object, \"Time\": object})\n",
    "\n",
    "    with open(output_dir + \"hdfs_log_templates.json\", \"r\") as f:\n",
    "        event_num = json.load(f)\n",
    "    df[\"EventId\"] = df[\"EventId\"].apply(lambda x: event_num.get(x, -1))\n",
    "\n",
    "    data_dict = defaultdict(list) #preserve insertion order of items\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        blkId_list = re.findall(r'(blk_-?\\d+)', row['Content'])\n",
    "        blkId_set = set(blkId_list)\n",
    "        for blk_Id in blkId_set:\n",
    "            data_dict[blk_Id].append(row[\"EventId\"])\n",
    "\n",
    "    data_df = pd.DataFrame(list(data_dict.items()), columns=['BlockId', 'EventSequence'])\n",
    "    data_df.to_csv(log_sequence_file, index=None)\n",
    "    print(\"hdfs sampling done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test(hdfs_sequence_file, n=None, ratio=0.3):\n",
    "    blk_label_dict = {}\n",
    "    blk_label_file = os.path.join(input_dir, \"anomaly_label.csv\")\n",
    "    blk_df = pd.read_csv(blk_label_file)\n",
    "    for _ , row in tqdm(blk_df.iterrows()):\n",
    "        blk_label_dict[row[\"BlockId\"]] = 1 if row[\"Label\"] == \"Anomaly\" else 0\n",
    "\n",
    "    seq = pd.read_csv(hdfs_sequence_file)\n",
    "    seq[\"Label\"] = seq[\"BlockId\"].apply(lambda x: blk_label_dict.get(x)) #add label to the sequence of each blockid\n",
    "\n",
    "    normal_seq = seq[seq[\"Label\"] == 0][\"EventSequence\"]\n",
    "    normal_seq = normal_seq.sample(frac=1, random_state=20) # shuffle normal data\n",
    "\n",
    "    abnormal_seq = seq[seq[\"Label\"] == 1][\"EventSequence\"]\n",
    "    normal_len, abnormal_len = len(normal_seq), len(abnormal_seq)\n",
    "    train_len = n if n else int(normal_len * ratio)\n",
    "    print(\"normal size {0}, abnormal size {1}, training size {2}\".format(normal_len, abnormal_len, train_len))\n",
    "\n",
    "    train = normal_seq.iloc[:train_len]\n",
    "    test_normal = normal_seq.iloc[train_len:]\n",
    "    test_abnormal = abnormal_seq\n",
    "\n",
    "    df_to_file(train, output_dir + \"train\")\n",
    "    df_to_file(test_normal, output_dir + \"test_normal\")\n",
    "    df_to_file(test_abnormal, output_dir + \"test_abnormal\")\n",
    "    print(\"generate train test data done\")\n",
    "\n",
    "\n",
    "def df_to_file(df, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for _, row in df.items():\n",
    "            f.write(' '.join([str(ele) for ele in eval(row)]))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: ../Datasets/hdfs/HDFS.log\n",
      "Total size after encoding is 3746105 3746106\n",
      "Parsing done. [Time taken: 0:03:41.114909]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. parse HDFS log\n",
    "    log_format = '<Date> <Time> <Pid> <Level> <Component>: <Content>'  # HDFS log format\n",
    "    parser(input_dir, output_dir, log_file, log_format, 'drain')\n",
    "    #mapping()\n",
    "    #hdfs_sampling(log_structured_file)\n",
    "    #generate_train_test(log_sequence_file, n=4855)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "182fe3b26f900d191053996287a94e64fbebd9ca1b4c644d38d8e1b9f7b6df61"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
