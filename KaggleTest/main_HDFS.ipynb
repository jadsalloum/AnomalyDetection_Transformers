{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install plotly\n",
    "!pip install tqdm\n",
    "'''\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import LSTM, GRU,SimpleRNN\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get [log key, delta time] as input for deeplog\n",
    "input_dir  = '../Datasets/hdfs/'\n",
    "output_dir = '../output/hdfs/'  # The output directory of parsing results\n",
    "log_file   = \"HDFS.log\"  # The input log file name\n",
    "\n",
    "log_structured_file = output_dir + log_file + \"_structured.csv\"\n",
    "log_labeled_file = output_dir + log_file + \"_labeled.csv\"\n",
    "log_sequence_file = output_dir + \"hdfs_sequence.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring TPU's as we will be using Bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train = logFile2DataFrame('../Datasets/HDFS/HDFS.log')\n",
    "#validation = pd.read_csv('../Datasets/Sentiment/validation.csv')\n",
    "#test = pd.read_csv('../Datasets/Sentiment/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def load_data(log_format):\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    df_log = log_to_dataframe(os.path.join(input_dir, log_file), regex, headers, log_format)\n",
    "    df_log.to_csv(log_structured_file, index=None)\n",
    "    return df_log\n",
    "\n",
    "def log_to_dataframe( log_file, regex, headers, logformat):\n",
    "    \"\"\" Function to transform log file to dataframe\n",
    "    \"\"\"\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    cnt = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            cnt += 1\n",
    "            try:\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "            except Exception as e:\n",
    "                # print(\"\\n\", line)\n",
    "                # print(e)\n",
    "                pass\n",
    "    print(\"Total size after encoding is\", linecount, cnt)\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "def generate_logformat_regex( logformat):\n",
    "    \"\"\" Function to generate regular expression to split log messages\n",
    "    \"\"\"\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', logformat)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += '(?P<%s>.*?)' % header\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size after encoding is 2270605 2270605\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pid</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>35</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.FSNamesystem</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 1 for block blk_-1608999687919...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId    Date    Time  Pid Level                     Component  \\\n",
       "0       1  081109  203518  143  INFO      dfs.DataNode$DataXceiver   \n",
       "1       2  081109  203518   35  INFO              dfs.FSNamesystem   \n",
       "2       3  081109  203519  143  INFO      dfs.DataNode$DataXceiver   \n",
       "3       4  081109  203519  145  INFO      dfs.DataNode$DataXceiver   \n",
       "4       5  081109  203519  145  INFO  dfs.DataNode$PacketResponder   \n",
       "\n",
       "                                             Content  \n",
       "0  Receiving block blk_-1608999687919862906 src: ...  \n",
       "1  BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...  \n",
       "2  Receiving block blk_-1608999687919862906 src: ...  \n",
       "3  Receiving block blk_-1608999687919862906 src: ...  \n",
       "4  PacketResponder 1 for block blk_-1608999687919...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_format = '<Date> <Time> <Pid> <Level> <Component>: <Content>'  # HDFS log format\n",
    "train = load_data(log_format)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../output/hdfs/HDFS.log_structured.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2270605it [02:00, 18839.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>EventSequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>[Receiving block blk_-1608999687919862906 src:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>[Receiving block blk_7503483334202473044 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>[Receiving block blk_-3544583377289625738 src:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>[Receiving block blk_-9073992586687739851 src:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>[Receiving block blk_7854771516489510256 src: ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BlockId                                      EventSequence\n",
       "0  blk_-1608999687919862906  [Receiving block blk_-1608999687919862906 src:...\n",
       "1   blk_7503483334202473044  [Receiving block blk_7503483334202473044 src: ...\n",
       "2  blk_-3544583377289625738  [Receiving block blk_-3544583377289625738 src:...\n",
       "3  blk_-9073992586687739851  [Receiving block blk_-9073992586687739851 src:...\n",
       "4   blk_7854771516489510256  [Receiving block blk_7854771516489510256 src: ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def Merge_Sequences_by_BlockId(log_file, window='session'):\n",
    "    assert window == 'session', \"Only window=session is supported for HDFS dataset.\"\n",
    "    print(\"Loading\", log_file)\n",
    "    df = pd.read_csv(log_file, engine='c',\n",
    "            na_filter=False, memory_map=True, dtype={'Date':object, \"Time\": object})\n",
    "\n",
    "    df['BlockId']= \"NULL\"\n",
    "    \n",
    "    data_dict = defaultdict(list) #preserve insertion order of items\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        blkId_list = re.findall(r'(blk_-?\\d+)', row['Content'])\n",
    "        blkId_set = set(blkId_list)\n",
    "        for blk_Id in blkId_set:\n",
    "            data_dict[blk_Id].append(row[\"Content\"])\n",
    "\n",
    "    data_df = pd.DataFrame(list(data_dict.items()), columns=['BlockId', 'EventSequence'])  \n",
    "    data_df.to_csv(log_sequence_file, index=None)\n",
    "    return data_df\n",
    "\n",
    "train= Merge_Sequences_by_BlockId(log_structured_file)\n",
    "train.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "575061it [00:32, 17926.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate sequence and labels done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>EventSequence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>['Receiving block blk_-1608999687919862906 src...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>['Receiving block blk_7503483334202473044 src:...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>['Receiving block blk_-3544583377289625738 src...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>['Receiving block blk_-9073992586687739851 src...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>['Receiving block blk_7854771516489510256 src:...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BlockId  \\\n",
       "0  blk_-1608999687919862906   \n",
       "1   blk_7503483334202473044   \n",
       "2  blk_-3544583377289625738   \n",
       "3  blk_-9073992586687739851   \n",
       "4   blk_7854771516489510256   \n",
       "\n",
       "                                       EventSequence  Label  \n",
       "0  ['Receiving block blk_-1608999687919862906 src...    0.0  \n",
       "1  ['Receiving block blk_7503483334202473044 src:...    0.0  \n",
       "2  ['Receiving block blk_-3544583377289625738 src...    1.0  \n",
       "3  ['Receiving block blk_-9073992586687739851 src...    0.0  \n",
       "4  ['Receiving block blk_7854771516489510256 src:...    0.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Assign_Labels_to_Sequence(hdfs_sequence_file, n=None, ratio=0.3):\n",
    "    blk_label_dict = {}\n",
    "    blk_label_file = os.path.join(input_dir, \"anomaly_label.csv\")\n",
    "    blk_df = pd.read_csv(blk_label_file)\n",
    "    for _ , row in tqdm(blk_df.iterrows()):\n",
    "        blk_label_dict[row[\"BlockId\"]] = 1 if row[\"Label\"] == \"Anomaly\" else 0\n",
    "\n",
    "    seq = pd.read_csv(hdfs_sequence_file)\n",
    "    seq[\"Label\"] = seq[\"BlockId\"].apply(lambda x: blk_label_dict.get(x)) #add label to the sequence of each blockid\n",
    "\n",
    "    print(\"Generate sequence and labels done\")\n",
    "    seq.to_csv(log_labeled_file, index=None)\n",
    "    return seq\n",
    "\n",
    "train = Assign_Labels_to_Sequence(log_sequence_file)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blk_-7718737206603036'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['BlockId'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>EventSequence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109185</th>\n",
       "      <td>blk_-7718737206603036</td>\n",
       "      <td>['Deleting block blk_-7718737206603036']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      BlockId                             EventSequence  Label\n",
       "109185  blk_-7718737206603036  ['Deleting block blk_-7718737206603036']    NaN"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train['BlockId'].eq('blk_-7718737206603036')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove any NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109185, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.dropna()\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the maximum number of words in a Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1787"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['EventSequence'].apply(lambda x:len(str(x).split())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing a function for getting auc score for validation\n",
    "\n",
    "def roc_auc(predictions,target):\n",
    "    '''\n",
    "    This methods returns the AUC Score when given the Predictions\n",
    "    and Labels\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.EventSequence.values, train.Label.values, \n",
    "                                                  stratify=train.Label.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 10\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "#zero pad the sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "xtrain_pad = pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    12,     13,      5, ...,     63,    141,      9],\n",
       "       [    12,     13,      5, ...,     52,    137,      9],\n",
       "       [ 45108,    114,    112, ...,    353,      1, 194698],\n",
       "       ...,\n",
       "       [    12,     13,      5, ...,     95,    150,      9],\n",
       "       [    12,     13,      5, ...,     76,     79,      9],\n",
       "       [    12,     13,      5, ...,     73,    124,      9]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xvalid_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 10)            2611890   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 10)                210       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,612,111\n",
      "Trainable params: 2,612,111\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: total: 406 ms\n",
      "Wall time: 560 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # A simpleRNN without any pretrained embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     10,\n",
    "                     input_length=max_len))\n",
    "    model.add(SimpleRNN(10))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1365/1365 [==============================] - 68s 49ms/step - loss: 0.1317 - accuracy: 0.9700\n",
      "Epoch 2/5\n",
      "1365/1365 [==============================] - 63s 46ms/step - loss: 0.0946 - accuracy: 0.9774\n",
      "Epoch 3/5\n",
      "1365/1365 [==============================] - 64s 47ms/step - loss: 0.0801 - accuracy: 0.9824\n",
      "Epoch 4/5\n",
      "1365/1365 [==============================] - 65s 48ms/step - loss: 0.0758 - accuracy: 0.9843\n",
      "Epoch 5/5\n",
      "1365/1365 [==============================] - 71s 52ms/step - loss: 0.0742 - accuracy: 0.9849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27ed059b5e0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64*strategy.num_replicas_in_sync) #Multiplying by Strategy to run on TPU's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683/683 [==============================] - 2s 3ms/step\n",
      "Auc: 0.78%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model = []\n",
    "scores_model.append({'Model': 'SimpleRNN','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7,\n",
       "  8,\n",
       "  40,\n",
       "  42,\n",
       "  41,\n",
       "  101,\n",
       "  43,\n",
       "  44,\n",
       "  46,\n",
       "  100,\n",
       "  53,\n",
       "  2720,\n",
       "  32,\n",
       "  45,\n",
       "  2721,\n",
       "  1,\n",
       "  200037,\n",
       "  19,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  17,\n",
       "  2,\n",
       "  3,\n",
       "  59,\n",
       "  120,\n",
       "  76286,\n",
       "  18,\n",
       "  2,\n",
       "  3,\n",
       "  59,\n",
       "  120,\n",
       "  9,\n",
       "  19,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  17,\n",
       "  2,\n",
       "  3,\n",
       "  76,\n",
       "  110,\n",
       "  115162,\n",
       "  18,\n",
       "  2,\n",
       "  3,\n",
       "  76,\n",
       "  110,\n",
       "  9,\n",
       "  19,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  17,\n",
       "  2,\n",
       "  3,\n",
       "  76,\n",
       "  110,\n",
       "  133584,\n",
       "  18,\n",
       "  2,\n",
       "  3,\n",
       "  76,\n",
       "  110,\n",
       "  9,\n",
       "  22,\n",
       "  32,\n",
       "  16,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  23,\n",
       "  21,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  11,\n",
       "  10,\n",
       "  29,\n",
       "  20,\n",
       "  2,\n",
       "  3,\n",
       "  59,\n",
       "  222,\n",
       "  22,\n",
       "  38,\n",
       "  16,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  23,\n",
       "  21,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  11,\n",
       "  10,\n",
       "  29,\n",
       "  20,\n",
       "  2,\n",
       "  3,\n",
       "  76,\n",
       "  219,\n",
       "  7,\n",
       "  8,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  2,\n",
       "  3,\n",
       "  59,\n",
       "  120,\n",
       "  6,\n",
       "  12,\n",
       "  13,\n",
       "  5,\n",
       "  1,\n",
       "  10163,\n",
       "  10,\n",
       "  30,\n",
       "  7,\n",
       "  8,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  2,\n",
       "  3,\n",
       "  76,\n",
       "  110,\n",
       "  6,\n",
       "  12,\n",
       "  13,\n",
       "  5,\n",
       "  1,\n",
       "  10163,\n",
       "  10,\n",
       "  30,\n",
       "  7,\n",
       "  8,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  2,\n",
       "  3,\n",
       "  50,\n",
       "  138,\n",
       "  6,\n",
       "  12,\n",
       "  13,\n",
       "  5,\n",
       "  1,\n",
       "  10163,\n",
       "  10,\n",
       "  30,\n",
       "  22,\n",
       "  47,\n",
       "  16,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  23,\n",
       "  21,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  11,\n",
       "  10,\n",
       "  29,\n",
       "  20,\n",
       "  2,\n",
       "  3,\n",
       "  76,\n",
       "  219,\n",
       "  14,\n",
       "  3,\n",
       "  59,\n",
       "  120,\n",
       "  6,\n",
       "  31,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  59,\n",
       "  222,\n",
       "  14,\n",
       "  3,\n",
       "  59,\n",
       "  120,\n",
       "  6,\n",
       "  36,\n",
       "  33,\n",
       "  35,\n",
       "  37,\n",
       "  1,\n",
       "  10163,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  135,\n",
       "  34,\n",
       "  14,\n",
       "  3,\n",
       "  59,\n",
       "  120,\n",
       "  6,\n",
       "  31,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  5,\n",
       "  2,\n",
       "  15,\n",
       "  83,\n",
       "  175,\n",
       "  14,\n",
       "  3,\n",
       "  59,\n",
       "  120,\n",
       "  6,\n",
       "  31,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  59,\n",
       "  222,\n",
       "  14,\n",
       "  3,\n",
       "  76,\n",
       "  110,\n",
       "  6,\n",
       "  31,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  5,\n",
       "  2,\n",
       "  15,\n",
       "  83,\n",
       "  175,\n",
       "  14,\n",
       "  3,\n",
       "  50,\n",
       "  138,\n",
       "  6,\n",
       "  36,\n",
       "  33,\n",
       "  35,\n",
       "  37,\n",
       "  1,\n",
       "  10163,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  135,\n",
       "  34,\n",
       "  14,\n",
       "  3,\n",
       "  59,\n",
       "  120,\n",
       "  6,\n",
       "  31,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  59,\n",
       "  222,\n",
       "  14,\n",
       "  3,\n",
       "  76,\n",
       "  110,\n",
       "  6,\n",
       "  36,\n",
       "  33,\n",
       "  35,\n",
       "  37,\n",
       "  1,\n",
       "  10163,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  76,\n",
       "  110,\n",
       "  34,\n",
       "  14,\n",
       "  3,\n",
       "  50,\n",
       "  138,\n",
       "  6,\n",
       "  31,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  5,\n",
       "  2,\n",
       "  15,\n",
       "  83,\n",
       "  175,\n",
       "  14,\n",
       "  3,\n",
       "  59,\n",
       "  120,\n",
       "  6,\n",
       "  31,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  59,\n",
       "  222,\n",
       "  14,\n",
       "  3,\n",
       "  50,\n",
       "  138,\n",
       "  6,\n",
       "  31,\n",
       "  4,\n",
       "  1,\n",
       "  10163,\n",
       "  5,\n",
       "  2,\n",
       "  15,\n",
       "  83,\n",
       "  175,\n",
       "  14,\n",
       "  3,\n",
       "  76,\n",
       "  110,\n",
       "  6,\n",
       "  36,\n",
       "  33,\n",
       "  35,\n",
       "  37,\n",
       "  1,\n",
       "  10163,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  49,\n",
       "  121,\n",
       "  34,\n",
       "  7,\n",
       "  8,\n",
       "  27,\n",
       "  1,\n",
       "  10163,\n",
       "  12,\n",
       "  13,\n",
       "  5,\n",
       "  28,\n",
       "  11,\n",
       "  2,\n",
       "  3,\n",
       "  50,\n",
       "  138,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  27,\n",
       "  1,\n",
       "  10163,\n",
       "  12,\n",
       "  13,\n",
       "  5,\n",
       "  28,\n",
       "  11,\n",
       "  2,\n",
       "  3,\n",
       "  59,\n",
       "  120,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  27,\n",
       "  1,\n",
       "  10163,\n",
       "  12,\n",
       "  13,\n",
       "  5,\n",
       "  28,\n",
       "  11,\n",
       "  2,\n",
       "  3,\n",
       "  76,\n",
       "  110,\n",
       "  9]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_seq[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [07:17, 5022.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../Datasets/glove840b300dtxt/glove.840B.300d.txt','r',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray([float(val) for val in values[1:]])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261188/261188 [00:00<00:00, 637175.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 10, 300)           78356700  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 10)                12440     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 78,369,151\n",
      "Trainable params: 12,451\n",
      "Non-trainable params: 78,356,700\n",
      "_________________________________________________________________\n",
      "CPU times: total: 4.55 s\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    \n",
    "    # A simple LSTM with glove embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "\n",
    "    model.add(LSTM(10, dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1365/1365 [==============================] - 24s 15ms/step - loss: 0.1389 - accuracy: 0.9671\n",
      "Epoch 2/5\n",
      "1365/1365 [==============================] - 23s 17ms/step - loss: 0.1229 - accuracy: 0.9712\n",
      "Epoch 3/5\n",
      "1365/1365 [==============================] - 23s 17ms/step - loss: 0.1219 - accuracy: 0.9713\n",
      "Epoch 4/5\n",
      "1365/1365 [==============================] - 23s 17ms/step - loss: 0.1217 - accuracy: 0.9713\n",
      "Epoch 5/5\n",
      "1365/1365 [==============================] - 24s 17ms/step - loss: 0.1213 - accuracy: 0.9713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27ebe6fa8c0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain,epochs=5, batch_size=64*strategy.num_replicas_in_sync)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683/683 [==============================] - 3s 4ms/step\n",
      "Auc: 0.81%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model.append({'Model': 'LSTM','AUC_Score': roc_auc(scores,yvalid)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit - GRU's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 10, 300)           78356700  \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 10, 300)          0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 10)                9360      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 78,366,071\n",
      "Trainable params: 9,371\n",
      "Non-trainable params: 78,356,700\n",
      "_________________________________________________________________\n",
      "CPU times: total: 4.69 s\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # GRU with glove embeddings and two dense layers\n",
    "     model = Sequential()\n",
    "     model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "     model.add(SpatialDropout1D(0.3))\n",
    "     model.add(GRU(10))\n",
    "     model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "     model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])   \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1365/1365 [==============================] - 16s 9ms/step - loss: 0.1367 - accuracy: 0.9675\n",
      "Epoch 2/5\n",
      "1365/1365 [==============================] - 14s 10ms/step - loss: 0.1215 - accuracy: 0.9713\n",
      "Epoch 3/5\n",
      "1365/1365 [==============================] - 12s 9ms/step - loss: 0.1207 - accuracy: 0.9713\n",
      "Epoch 4/5\n",
      "1365/1365 [==============================] - 11s 8ms/step - loss: 0.1200 - accuracy: 0.9714\n",
      "Epoch 5/5\n",
      "1365/1365 [==============================] - 12s 9ms/step - loss: 0.1199 - accuracy: 0.9715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28158011660>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64*strategy.num_replicas_in_sync)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683/683 [==============================] - 3s 3ms/step\n",
      "Auc: 0.81%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model.append({'Model': 'GRU','AUC_Score': roc_auc(scores,yvalid)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Model': 'SimpleRNN', 'AUC_Score': 0.7754204216597802},\n",
       " {'Model': 'LSTM', 'AUC_Score': 0.8143838216079735},\n",
       " {'Model': 'GRU', 'AUC_Score': 0.8145220293622619}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Directional RNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 10, 300)           78356700  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 20)               24880     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 78,381,601\n",
      "Trainable params: 24,901\n",
      "Non-trainable params: 78,356,700\n",
      "_________________________________________________________________\n",
      "CPU times: total: 4.31 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # A simple bidirectional LSTM with glove embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "    model.add(Bidirectional(LSTM(10, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1365/1365 [==============================] - 39s 23ms/step - loss: 0.1323 - accuracy: 0.9666\n",
      "Epoch 2/5\n",
      "1365/1365 [==============================] - 34s 25ms/step - loss: 0.1218 - accuracy: 0.9713\n",
      "Epoch 3/5\n",
      "1365/1365 [==============================] - 44s 32ms/step - loss: 0.1212 - accuracy: 0.9713\n",
      "Epoch 4/5\n",
      "1365/1365 [==============================] - 54s 40ms/step - loss: 0.1207 - accuracy: 0.9713\n",
      "Epoch 5/5\n",
      "1365/1365 [==============================] - 68s 50ms/step - loss: 0.1204 - accuracy: 0.9713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x281585139d0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64*strategy.num_replicas_in_sync)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683/683 [==============================] - 5s 6ms/step\n",
      "Auc: 0.82%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model.append({'Model': 'Bi-directional LSTM','AUC_Score': roc_auc(scores,yvalid)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code need to be implemented still"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Jinja2 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from Jinja2) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (5.4.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from nbformat) (4.10.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from nbformat) (5.3.0)\n",
      "Requirement already satisfied: fastjsonschema in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from nbformat) (2.15.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from nbformat) (4.6.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.18.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from jupyter-core->nbformat) (304)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (5.4.0)\n",
      "Requirement already satisfied: fastjsonschema in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from nbformat>=4.2.0) (2.15.3)\n",
      "Requirement already satisfied: jupyter-core in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from nbformat>=4.2.0) (4.10.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from nbformat>=4.2.0) (4.6.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from nbformat>=4.2.0) (5.3.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0) (21.4.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\jad\\ai_projects\\anomalydetection_transformers\\.venv\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0) (304)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_73725_row0_col1 {\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_73725_row1_col1, #T_73725_row2_col1 {\n",
       "  background-color: #084184;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_73725_row3_col1 {\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_73725\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_73725_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_73725_level0_col1\" class=\"col_heading level0 col1\" >AUC_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_73725_level0_row0\" class=\"row_heading level0 row0\" >3</th>\n",
       "      <td id=\"T_73725_row0_col0\" class=\"data row0 col0\" >Bi-directional LSTM</td>\n",
       "      <td id=\"T_73725_row0_col1\" class=\"data row0 col1\" >0.817154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73725_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
       "      <td id=\"T_73725_row1_col0\" class=\"data row1 col0\" >GRU</td>\n",
       "      <td id=\"T_73725_row1_col1\" class=\"data row1 col1\" >0.814522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73725_level0_row2\" class=\"row_heading level0 row2\" >1</th>\n",
       "      <td id=\"T_73725_row2_col0\" class=\"data row2 col0\" >LSTM</td>\n",
       "      <td id=\"T_73725_row2_col1\" class=\"data row2 col1\" >0.814384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_73725_level0_row3\" class=\"row_heading level0 row3\" >0</th>\n",
       "      <td id=\"T_73725_row3_col0\" class=\"data row3 col0\" >SimpleRNN</td>\n",
       "      <td id=\"T_73725_row3_col1\" class=\"data row3 col1\" >0.775420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x281603126e0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install Jinja2\n",
    "!pip install nbformat\n",
    "!pip install  \"nbformat>=4.2.0\"\n",
    "\n",
    "#Visualization of Results obtained from various Deep learning models\n",
    "results = pd.DataFrame(scores_model).sort_values(by='AUC_Score',ascending=False)\n",
    "results.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "### ================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dependencies\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for pycocotools (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [14 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-310\n",
      "      creating build\\lib.win-amd64-cpython-310\\pycocotools\n",
      "      copying pycocotools\\coco.py -> build\\lib.win-amd64-cpython-310\\pycocotools\n",
      "      copying pycocotools\\cocoeval.py -> build\\lib.win-amd64-cpython-310\\pycocotools\n",
      "      copying pycocotools\\mask.py -> build\\lib.win-amd64-cpython-310\\pycocotools\n",
      "      copying pycocotools\\__init__.py -> build\\lib.win-amd64-cpython-310\\pycocotools\n",
      "      running build_ext\n",
      "      skipping 'pycocotools\\_mask.c' Cython extension (up-to-date)\n",
      "      building 'pycocotools._mask' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pycocotools\n",
      "ERROR: Could not build wheels for pycocotools, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "c:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow_text\\python\\ops\\_regex_split_ops.so not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\KaggleTest\\main_HDFS.ipynb Cell 53'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000071?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000071?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mhub\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000071?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtext\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000071?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mofficial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnlp\u001b[39;00m \u001b[39mimport\u001b[39;00m optimization  \u001b[39m# to create AdamW optimizer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000071?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow_text\\__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpybinds\u001b[39;00m \u001b[39mimport\u001b[39;00m tflite_registrar\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow_text\\python\\keras\\__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mall_util\u001b[39;00m \u001b[39mimport\u001b[39;00m remove_undocumented\n\u001b[0;32m     20\u001b[0m \u001b[39m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Public symbols in the \"tensorflow_text.layers\" package.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m _allowed_symbols \u001b[39m=\u001b[39m [\n\u001b[0;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlayers\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     26\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow_text\\python\\keras\\layers\\__init__.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtodense\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenization_layers\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[39m# Public symbols in the \"tensorflow_text.layers\" package.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m _allowed_symbols \u001b[39m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mToDense\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mUnicodeScriptTokenizer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mWhitespaceTokenizer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mWordpieceTokenizer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow_text\\python\\keras\\layers\\tokenization_layers.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m lookup_ops\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mragged\u001b[39;00m \u001b[39mimport\u001b[39;00m ragged_conversion_ops\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m unicode_script_tokenizer\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m whitespace_tokenizer\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m wordpiece_tokenizer\n",
      "File \u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow_text\\python\\ops\\__init__.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpybinds\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpywrap_fast_wordpiece_tokenizer_model_builder\u001b[39;00m \u001b[39mimport\u001b[39;00m build_fast_wordpiece_model\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbert_tokenizer\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcreate_feature_bitmask_op\u001b[39;00m \u001b[39mimport\u001b[39;00m create_feature_bitmask\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfast_wordpiece_tokenizer\u001b[39;00m \u001b[39mimport\u001b[39;00m FastWordpieceTokenizer\n",
      "File \u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow_text\\python\\ops\\bert_tokenizer.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m array_ops\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m string_ops\n\u001b[1;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m regex_split_ops\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnormalize_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m case_fold_utf8\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnormalize_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m normalize_utf8\n",
      "File \u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow_text\\python\\ops\\regex_split_ops.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m load_library\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m resource_loader\n\u001b[1;32m---> 24\u001b[0m gen_regex_split_ops \u001b[39m=\u001b[39m load_library\u001b[39m.\u001b[39;49mload_op_library(resource_loader\u001b[39m.\u001b[39;49mget_path_to_datafile(\u001b[39m'\u001b[39;49m\u001b[39m_regex_split_ops.so\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m splitter\n\u001b[0;32m     28\u001b[0m \u001b[39m# pylint: disable= redefined-builtin\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py:54\u001b[0m, in \u001b[0;36mload_op_library\u001b[1;34m(library_filename)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mload_op_library\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_op_library\u001b[39m(library_filename):\n\u001b[0;32m     33\u001b[0m   \u001b[39m\"\"\"Loads a TensorFlow plugin, containing custom ops and kernels.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[39m  Pass \"library_filename\" to a platform-specific mechanism for dynamically\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m    RuntimeError: when unable to load the library or get the python wrappers.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m   lib_handle \u001b[39m=\u001b[39m py_tf\u001b[39m.\u001b[39;49mTF_LoadLibrary(library_filename)\n\u001b[0;32m     55\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     wrappers \u001b[39m=\u001b[39m _pywrap_python_op_gen\u001b[39m.\u001b[39mGetPythonWrappers(\n\u001b[0;32m     57\u001b[0m         py_tf\u001b[39m.\u001b[39mTF_GetOpList(lib_handle))\n",
      "\u001b[1;31mNotFoundError\u001b[0m: c:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow_text\\python\\ops\\_regex_split_ops.so not found"
     ]
    }
   ],
   "source": [
    "!pip install -q -U \"tensorflow-text==2.8.*\"\n",
    "!pip install -q tf-models-official==2.7.0\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>EventSequence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>['Receiving block blk_-1608999687919862906 src...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>['Receiving block blk_7503483334202473044 src:...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>['Receiving block blk_-3544583377289625738 src...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>['Receiving block blk_-9073992586687739851 src...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>['Receiving block blk_7854771516489510256 src:...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BlockId  \\\n",
       "0  blk_-1608999687919862906   \n",
       "1   blk_7503483334202473044   \n",
       "2  blk_-3544583377289625738   \n",
       "3  blk_-9073992586687739851   \n",
       "4   blk_7854771516489510256   \n",
       "\n",
       "                                       EventSequence  Label  \n",
       "0  ['Receiving block blk_-1608999687919862906 src...    0.0  \n",
       "1  ['Receiving block blk_7503483334202473044 src:...    0.0  \n",
       "2  ['Receiving block blk_-3544583377289625738 src...    1.0  \n",
       "3  ['Receiving block blk_-9073992586687739851 src...    0.0  \n",
       "4  ['Receiving block blk_7854771516489510256 src:...    0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(log_labeled_file)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (65511,)\n",
      "y_train shape: (65511,)\n",
      "X_test shape: (21838,)\n",
      "y_test shape: (21838,)\n",
      "X_val shape: (21837,)\n",
      "y_val shape: (21837,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set aside 20% of train and test data for evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.EventSequence, train.Label,  test_size=0.2, shuffle = True, random_state = 8)\n",
    "\n",
    "# Use the same function above for the validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state= 8) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n",
    "print(\"X_val shape: {}\".format(X_val.shape))\n",
    "print(\"y_val shape: {}\".format(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    \"\"\"\n",
    "    Encoder for encoding the text into sequence of integers for BERT Input\n",
    "    \"\"\"\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMP DATA FOR CONFIG\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=119547, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086d8759b3f54fa88441baa2dcda0b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/517M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 10000\n",
    "###train['EventSequence'].apply(lambda x:len(str(x).split())).max()\n",
    "\n",
    "# First load the real tokenizer\n",
    "model_name= 'distilbert-base-multilingual-cased'\n",
    "model = transformers.DistilBertModel.from_pretrained(model_name)\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(str(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"77276     ['BLOCK* NameSystem.allocateBlock: /user/root/...\\n52708     ['BLOCK* NameSystem.allocateBlock: /user/root/...\\n14146     ['Receiving block blk_-2354440619947518714 src...\\n93357     ['BLOCK* NameSystem.allocateBlock: /user/root/...\\n32688     ['Receiving block blk_-2166134416691254576 src...\\n                                ...                        \\n45701     ['BLOCK* NameSystem.allocateBlock: /user/root/...\\n21200     ['BLOCK* NameSystem.allocateBlock: /user/root/...\\n108727    ['BLOCK* NameSystem.allocateBlock: /user/root/...\\n8797      ['BLOCK* NameSystem.allocateBlock: /user/root/...\\n81441     ['BLOCK* NameSystem.allocateBlock: /user/root/...\\nName: EventSequence, Length: 65511, dtype: object\""
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer(str(X_train), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "x_valid = tokenizer(str(X_val), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "#x_train = fast_encode(X_train.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "#x_valid = fast_encode(X_val.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "#x_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = y_train.values\n",
    "y_valid = y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 1 and 65511 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\KaggleTest\\main_HDFS.ipynb Cell 60'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=0'>1</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m (\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=1'>2</a>\u001b[0m     tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=2'>3</a>\u001b[0m     \u001b[39m.\u001b[39;49mfrom_tensor_slices((x_train, y_train))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=3'>4</a>\u001b[0m     \u001b[39m.\u001b[39mrepeat()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=4'>5</a>\u001b[0m     \u001b[39m.\u001b[39mshuffle(\u001b[39m2048\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=5'>6</a>\u001b[0m     \u001b[39m.\u001b[39mbatch(BATCH_SIZE)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=6'>7</a>\u001b[0m     \u001b[39m.\u001b[39mprefetch(AUTO)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=7'>8</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=9'>10</a>\u001b[0m valid_dataset \u001b[39m=\u001b[39m (\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=10'>11</a>\u001b[0m     tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=11'>12</a>\u001b[0m     \u001b[39m.\u001b[39mfrom_tensor_slices((x_valid, y_valid))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=14'>15</a>\u001b[0m     \u001b[39m.\u001b[39mprefetch(AUTO)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=15'>16</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=17'>18</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m (\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=18'>19</a>\u001b[0m     tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=19'>20</a>\u001b[0m     \u001b[39m.\u001b[39mfrom_tensor_slices(x_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=20'>21</a>\u001b[0m     \u001b[39m.\u001b[39mbatch(BATCH_SIZE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000058?line=21'>22</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:809\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=730'>731</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=731'>732</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_tensor_slices\u001b[39m(tensors, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=732'>733</a>\u001b[0m   \u001b[39m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=733'>734</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=734'>735</a>\u001b[0m \u001b[39m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=806'>807</a>\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=807'>808</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=808'>809</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m TensorSliceDataset(tensors, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4563\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4559'>4560</a>\u001b[0m batch_dim \u001b[39m=\u001b[39m tensor_shape\u001b[39m.\u001b[39mDimension(\n\u001b[0;32m   <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4560'>4561</a>\u001b[0m     tensor_shape\u001b[39m.\u001b[39mdimension_value(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_shape()[\u001b[39m0\u001b[39m]))\n\u001b[0;32m   <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4561'>4562</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m-> <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4562'>4563</a>\u001b[0m   batch_dim\u001b[39m.\u001b[39;49massert_is_compatible_with(\n\u001b[0;32m   <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4563'>4564</a>\u001b[0m       tensor_shape\u001b[39m.\u001b[39;49mDimension(\n\u001b[0;32m   <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4564'>4565</a>\u001b[0m           tensor_shape\u001b[39m.\u001b[39;49mdimension_value(t\u001b[39m.\u001b[39;49mget_shape()[\u001b[39m0\u001b[39;49m])))\n\u001b[0;32m   <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4566'>4567</a>\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mtensor_slice_dataset(\n\u001b[0;32m   <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4567'>4568</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensors,\n\u001b[0;32m   <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4568'>4569</a>\u001b[0m     output_shapes\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_shapes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure),\n\u001b[0;32m   <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4569'>4570</a>\u001b[0m     is_files\u001b[39m=\u001b[39mis_files,\n\u001b[0;32m   <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4570'>4571</a>\u001b[0m     metadata\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39mSerializeToString())\n\u001b[0;32m   <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/data/ops/dataset_ops.py?line=4571'>4572</a>\u001b[0m \u001b[39msuper\u001b[39m(TensorSliceDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(variant_tensor)\n",
      "File \u001b[1;32mc:\\Jad\\AI_Projects\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:298\u001b[0m, in \u001b[0;36mDimension.assert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/framework/tensor_shape.py?line=287'>288</a>\u001b[0m \u001b[39m\"\"\"Raises an exception if `other` is not compatible with this Dimension.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/framework/tensor_shape.py?line=288'>289</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/framework/tensor_shape.py?line=289'>290</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/framework/tensor_shape.py?line=294'>295</a>\u001b[0m \u001b[39m    is_compatible_with).\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/framework/tensor_shape.py?line=295'>296</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/framework/tensor_shape.py?line=296'>297</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_compatible_with(other):\n\u001b[1;32m--> <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/framework/tensor_shape.py?line=297'>298</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDimensions \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m are not compatible\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Jad/AI_Projects/AnomalyDetection_Transformers/.venv/lib/site-packages/tensorflow/python/framework/tensor_shape.py?line=298'>299</a>\u001b[0m                    (\u001b[39mself\u001b[39m, other))\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions 1 and 65511 are not compatible"
     ]
    }
   ],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=512):\n",
    "    \"\"\"\n",
    "    function for training the BERT model\n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    transformer_layer = (\n",
    "        transformers.TFDistilBertModel\n",
    "        .from_pretrained('distilbert-base-multilingual-cased')\n",
    "    )\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = x_train.shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
    "train_history_2 = model.fit(\n",
    "    valid_dataset.repeat(),\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs=EPOCHS*2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "182fe3b26f900d191053996287a94e64fbebd9ca1b4c644d38d8e1b9f7b6df61"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
