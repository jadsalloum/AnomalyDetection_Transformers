{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install plotly\n",
    "!pip install tqdm\n",
    "'''\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import LSTM, GRU,SimpleRNN\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get [log key, delta time] as input for deeplog\n",
    "input_dir  = '../Datasets/hdfs/'\n",
    "output_dir = '../output/hdfs/'  # The output directory of parsing results\n",
    "log_file   = \"HDFS.log\"  # The input log file name\n",
    "\n",
    "log_structured_file = output_dir + log_file + \"_structured.csv\"\n",
    "log_templates_file = output_dir + log_file + \"_templates.csv\"\n",
    "log_sequence_file = output_dir + \"hdfs_sequence.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring TPU's as we will be using Bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train = logFile2DataFrame('../Datasets/HDFS/HDFS.log')\n",
    "#validation = pd.read_csv('../Datasets/Sentiment/validation.csv')\n",
    "#test = pd.read_csv('../Datasets/Sentiment/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def load_data(log_format):\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    df_log = log_to_dataframe(os.path.join(input_dir, log_file), regex, headers, log_format)\n",
    "    return df_log\n",
    "\n",
    "def log_to_dataframe( log_file, regex, headers, logformat):\n",
    "    \"\"\" Function to transform log file to dataframe\n",
    "    \"\"\"\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    cnt = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            cnt += 1\n",
    "            try:\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "            except Exception as e:\n",
    "                # print(\"\\n\", line)\n",
    "                # print(e)\n",
    "                pass\n",
    "    print(\"Total size after encoding is\", linecount, cnt)\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "def generate_logformat_regex( logformat):\n",
    "    \"\"\" Function to generate regular expression to split log messages\n",
    "    \"\"\"\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', logformat)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += '(?P<%s>.*?)' % header\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size after encoding is 3746105 3746106\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pid</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>35</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.FSNamesystem</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 1 for block blk_-1608999687919...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId    Date    Time  Pid Level                     Component  \\\n",
       "0       1  081109  203518  143  INFO      dfs.DataNode$DataXceiver   \n",
       "1       2  081109  203518   35  INFO              dfs.FSNamesystem   \n",
       "2       3  081109  203519  143  INFO      dfs.DataNode$DataXceiver   \n",
       "3       4  081109  203519  145  INFO      dfs.DataNode$DataXceiver   \n",
       "4       5  081109  203519  145  INFO  dfs.DataNode$PacketResponder   \n",
       "\n",
       "                                             Content  \n",
       "0  Receiving block blk_-1608999687919862906 src: ...  \n",
       "1  BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...  \n",
       "2  Receiving block blk_-1608999687919862906 src: ...  \n",
       "3  Receiving block blk_-1608999687919862906 src: ...  \n",
       "4  PacketResponder 1 for block blk_-1608999687919...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_format = '<Date> <Time> <Pid> <Level> <Component>: <Content>'  # HDFS log format\n",
    "train = load_data(log_format)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3746105it [02:08, 29104.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>EventSequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>[Receiving block blk_-1608999687919862906 src:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>[Receiving block blk_7503483334202473044 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>[Receiving block blk_-3544583377289625738 src:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>[Receiving block blk_-9073992586687739851 src:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>[Receiving block blk_7854771516489510256 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blk_1717858812220360316</td>\n",
       "      <td>[Receiving block blk_1717858812220360316 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blk_-2519617320378473615</td>\n",
       "      <td>[Receiving block blk_-2519617320378473615 src:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>blk_7063315473424667801</td>\n",
       "      <td>[Receiving block blk_7063315473424667801 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>blk_8586544123689943463</td>\n",
       "      <td>[Receiving block blk_8586544123689943463 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>blk_2765344736980045501</td>\n",
       "      <td>[Receiving block blk_2765344736980045501 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>blk_-2900490557492272760</td>\n",
       "      <td>[Receiving block blk_-2900490557492272760 src:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>blk_-50273257731426871</td>\n",
       "      <td>[Receiving block blk_-50273257731426871 src: /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>blk_4394112519745907149</td>\n",
       "      <td>[Receiving block blk_4394112519745907149 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>blk_3640100967125688321</td>\n",
       "      <td>[Receiving block blk_3640100967125688321 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>blk_-40115644493265216</td>\n",
       "      <td>[Receiving block blk_-40115644493265216 src: /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>blk_-8531310335568756456</td>\n",
       "      <td>[Receiving block blk_-8531310335568756456 src:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>blk_-3409923645141256069</td>\n",
       "      <td>[Receiving block blk_-3409923645141256069 src:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>blk_3974948352784823938</td>\n",
       "      <td>[Receiving block blk_3974948352784823938 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>blk_5647760196018207394</td>\n",
       "      <td>[Receiving block blk_5647760196018207394 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>blk_-202775138379690649</td>\n",
       "      <td>[Receiving block blk_-202775138379690649 src: ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     BlockId  \\\n",
       "0   blk_-1608999687919862906   \n",
       "1    blk_7503483334202473044   \n",
       "2   blk_-3544583377289625738   \n",
       "3   blk_-9073992586687739851   \n",
       "4    blk_7854771516489510256   \n",
       "5    blk_1717858812220360316   \n",
       "6   blk_-2519617320378473615   \n",
       "7    blk_7063315473424667801   \n",
       "8    blk_8586544123689943463   \n",
       "9    blk_2765344736980045501   \n",
       "10  blk_-2900490557492272760   \n",
       "11    blk_-50273257731426871   \n",
       "12   blk_4394112519745907149   \n",
       "13   blk_3640100967125688321   \n",
       "14    blk_-40115644493265216   \n",
       "15  blk_-8531310335568756456   \n",
       "16  blk_-3409923645141256069   \n",
       "17   blk_3974948352784823938   \n",
       "18   blk_5647760196018207394   \n",
       "19   blk_-202775138379690649   \n",
       "\n",
       "                                        EventSequence  \n",
       "0   [Receiving block blk_-1608999687919862906 src:...  \n",
       "1   [Receiving block blk_7503483334202473044 src: ...  \n",
       "2   [Receiving block blk_-3544583377289625738 src:...  \n",
       "3   [Receiving block blk_-9073992586687739851 src:...  \n",
       "4   [Receiving block blk_7854771516489510256 src: ...  \n",
       "5   [Receiving block blk_1717858812220360316 src: ...  \n",
       "6   [Receiving block blk_-2519617320378473615 src:...  \n",
       "7   [Receiving block blk_7063315473424667801 src: ...  \n",
       "8   [Receiving block blk_8586544123689943463 src: ...  \n",
       "9   [Receiving block blk_2765344736980045501 src: ...  \n",
       "10  [Receiving block blk_-2900490557492272760 src:...  \n",
       "11  [Receiving block blk_-50273257731426871 src: /...  \n",
       "12  [Receiving block blk_4394112519745907149 src: ...  \n",
       "13  [Receiving block blk_3640100967125688321 src: ...  \n",
       "14  [Receiving block blk_-40115644493265216 src: /...  \n",
       "15  [Receiving block blk_-8531310335568756456 src:...  \n",
       "16  [Receiving block blk_-3409923645141256069 src:...  \n",
       "17  [Receiving block blk_3974948352784823938 src: ...  \n",
       "18  [Receiving block blk_5647760196018207394 src: ...  \n",
       "19  [Receiving block blk_-202775138379690649 src: ...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def hdfs_sampling(df, window='session'):\n",
    "    assert window == 'session', \"Only window=session is supported for HDFS dataset.\"\n",
    "    df['BlockId']= \"NULL\"\n",
    "    #for idx, row in tqdm(df.iterrows()):\n",
    "    #    df['Label'][idx] =  re.findall(r'(blk_-?\\d+)', row['Content'])\n",
    "    \n",
    "    data_dict = defaultdict(list) #preserve insertion order of items\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        blkId_list = re.findall(r'(blk_-?\\d+)', row['Content'])\n",
    "        blkId_set = set(blkId_list)\n",
    "        for blk_Id in blkId_set:\n",
    "            data_dict[blk_Id].append(row[\"Content\"])\n",
    "\n",
    "    data_df = pd.DataFrame(list(data_dict.items()), columns=['BlockId', 'EventSequence'])  \n",
    "    \n",
    "    return data_df\n",
    "\n",
    "aaa_df= hdfs_sampling(train)\n",
    "aaa_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa_df.to_csv(log_sequence_file, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test(hdfs_sequence_file, n=None, ratio=0.3):\n",
    "    blk_label_dict = {}\n",
    "    blk_label_file = os.path.join(input_dir, \"anomaly_label.csv\")\n",
    "    blk_df = pd.read_csv(blk_label_file)\n",
    "    for _ , row in tqdm(blk_df.iterrows()):\n",
    "        blk_label_dict[row[\"BlockId\"]] = 1 if row[\"Label\"] == \"Anomaly\" else 0\n",
    "\n",
    "    seq = hdfs_sequence_file.copy() #pd.read_csv(hdfs_sequence_file)\n",
    "    seq[\"Label\"] = seq[\"BlockId\"].apply(lambda x: blk_label_dict.get(x)) #add label to the sequence of each blockid\n",
    "\n",
    "    #normal_seq = seq[seq[\"Label\"] == 0][\"EventSequence\"]\n",
    "    #normal_seq = normal_seq.sample(frac=1, random_state=20) # shuffle normal data\n",
    "\n",
    "    #abnormal_seq = seq[seq[\"Label\"] == 1][\"EventSequence\"]\n",
    "    #normal_len, abnormal_len = len(normal_seq), len(abnormal_seq)\n",
    "    #train_len = n if n else int(normal_len * ratio)\n",
    "    #print(\"normal size {0}, abnormal size {1}, training size {2}\".format(normal_len, abnormal_len, train_len))\n",
    "\n",
    "    #train = seq.iloc[:train_len]\n",
    "    #test_normal = normal_seq.iloc[train_len:]\n",
    "    #test_abnormal = abnormal_seq\n",
    "\n",
    "    #df_to_file(train, output_dir + \"train\")\n",
    "    #df_to_file(test_normal, output_dir + \"test_normal\")\n",
    "    #df_to_file(test_abnormal, output_dir + \"test_abnormal\")\n",
    "    print(\"generate train test data done\")\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "575061it [00:17, 32079.76it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'BlockId'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\jads\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/indexes/base.py?line=3619'>3620</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\jads\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\jads\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'BlockId'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\jads\\AnomalyDetection_Transformers\\KaggleTest\\main_HDFS.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000023?line=0'>1</a>\u001b[0m new__df \u001b[39m=\u001b[39m generate_train_test(train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000023?line=1'>2</a>\u001b[0m new__df\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;32mc:\\jads\\AnomalyDetection_Transformers\\KaggleTest\\main_HDFS.ipynb Cell 8'\u001b[0m in \u001b[0;36mgenerate_train_test\u001b[1;34m(hdfs_sequence_file, n, ratio)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000022?line=5'>6</a>\u001b[0m     blk_label_dict[row[\u001b[39m\"\u001b[39m\u001b[39mBlockId\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m row[\u001b[39m\"\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAnomaly\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000022?line=7'>8</a>\u001b[0m seq \u001b[39m=\u001b[39m hdfs_sequence_file\u001b[39m.\u001b[39mcopy() \u001b[39m#pd.read_csv(hdfs_sequence_file)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000022?line=8'>9</a>\u001b[0m seq[\u001b[39m\"\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m seq[\u001b[39m\"\u001b[39;49m\u001b[39mBlockId\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: blk_label_dict\u001b[39m.\u001b[39mget(x)) \u001b[39m#add label to the sequence of each blockid\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000022?line=10'>11</a>\u001b[0m \u001b[39m#normal_seq = seq[seq[\"Label\"] == 0][\"EventSequence\"]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000022?line=11'>12</a>\u001b[0m \u001b[39m#normal_seq = normal_seq.sample(frac=1, random_state=20) # shuffle normal data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000022?line=12'>13</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000022?line=23'>24</a>\u001b[0m \u001b[39m#df_to_file(test_normal, output_dir + \"test_normal\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000022?line=24'>25</a>\u001b[0m \u001b[39m#df_to_file(test_abnormal, output_dir + \"test_abnormal\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000022?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgenerate train test data done\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\jads\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/frame.py?line=3502'>3503</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/frame.py?line=3503'>3504</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/frame.py?line=3504'>3505</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/frame.py?line=3505'>3506</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/frame.py?line=3506'>3507</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\jads\\AnomalyDetection_Transformers\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/indexes/base.py?line=3622'>3623</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/indexes/base.py?line=3623'>3624</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/indexes/base.py?line=3624'>3625</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/indexes/base.py?line=3625'>3626</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/indexes/base.py?line=3626'>3627</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/jads/AnomalyDetection_Transformers/.venv/lib/site-packages/pandas/core/indexes/base.py?line=3627'>3628</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'BlockId'"
     ]
    }
   ],
   "source": [
    "new__df = generate_train_test(train)\n",
    "new__df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[:12000,:]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the maximum number of words in a comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'].apply(lambda x:len(str(x).split())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing a function for getting auc score for validation\n",
    "\n",
    "def roc_auc(predictions,target):\n",
    "    '''\n",
    "    This methods returns the AUC Score when given the Predictions\n",
    "    and Labels\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.comment_text.values, train.toxic.values, \n",
    "                                                  stratify=train.toxic.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 1500\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "#zero pad the sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "xtrain_pad = pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # A simpleRNN without any pretrained embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     input_length=max_len))\n",
    "    model.add(SimpleRNN(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64*strategy.num_replicas_in_sync) #Multiplying by Strategy to run on TPU's"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4a2be7d9a8e225bb2e573ddefdac0c561e4de6358d3dd5bc426a91fd95e2205"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
