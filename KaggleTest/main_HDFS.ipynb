{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install plotly\n",
    "!pip install tqdm\n",
    "'''\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import LSTM, GRU,SimpleRNN\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get [log key, delta time] as input for deeplog\n",
    "input_dir  = '../Datasets/hdfs/'\n",
    "output_dir = '../output/hdfs/'  # The output directory of parsing results\n",
    "log_file   = \"HDFS.log\"  # The input log file name\n",
    "\n",
    "log_structured_file = output_dir + log_file + \"_structured.csv\"\n",
    "log_templates_file = output_dir + log_file + \"_templates.csv\"\n",
    "log_sequence_file = output_dir + \"hdfs_sequence.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring TPU's as we will be using Bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train = logFile2DataFrame('../Datasets/HDFS/HDFS.log')\n",
    "#validation = pd.read_csv('../Datasets/Sentiment/validation.csv')\n",
    "#test = pd.read_csv('../Datasets/Sentiment/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def load_data(log_format):\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    df_log = log_to_dataframe(os.path.join(input_dir, log_file), regex, headers, log_format)\n",
    "    return df_log\n",
    "\n",
    "def log_to_dataframe( log_file, regex, headers, logformat):\n",
    "    \"\"\" Function to transform log file to dataframe\n",
    "    \"\"\"\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    cnt = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            cnt += 1\n",
    "            try:\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "            except Exception as e:\n",
    "                # print(\"\\n\", line)\n",
    "                # print(e)\n",
    "                pass\n",
    "    print(\"Total size after encoding is\", linecount, cnt)\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "def generate_logformat_regex( logformat):\n",
    "    \"\"\" Function to generate regular expression to split log messages\n",
    "    \"\"\"\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', logformat)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += '(?P<%s>.*?)' % header\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size after encoding is 3746105 3746106\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineId</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pid</th>\n",
       "      <th>Level</th>\n",
       "      <th>Component</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>35</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.FSNamesystem</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 1 for block blk_-1608999687919...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LineId    Date    Time  Pid Level                     Component  \\\n",
       "0       1  081109  203518  143  INFO      dfs.DataNode$DataXceiver   \n",
       "1       2  081109  203518   35  INFO              dfs.FSNamesystem   \n",
       "2       3  081109  203519  143  INFO      dfs.DataNode$DataXceiver   \n",
       "3       4  081109  203519  145  INFO      dfs.DataNode$DataXceiver   \n",
       "4       5  081109  203519  145  INFO  dfs.DataNode$PacketResponder   \n",
       "\n",
       "                                             Content  \n",
       "0  Receiving block blk_-1608999687919862906 src: ...  \n",
       "1  BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...  \n",
       "2  Receiving block blk_-1608999687919862906 src: ...  \n",
       "3  Receiving block blk_-1608999687919862906 src: ...  \n",
       "4  PacketResponder 1 for block blk_-1608999687919...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_format = '<Date> <Time> <Pid> <Level> <Component>: <Content>'  # HDFS log format\n",
    "train = load_data(log_format)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\jads\\AnomalyDetection_Transformers\\KaggleTest\\main_HDFS.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000007?line=15'>16</a>\u001b[0m     data_df\u001b[39m.\u001b[39mto_csv(log_sequence_file, index\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000007?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data_df\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000007?line=18'>19</a>\u001b[0m train\u001b[39m=\u001b[39m hdfs_sampling(train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/main_HDFS.ipynb#ch0000007?line=19'>20</a>\u001b[0m train\u001b[39m.\u001b[39mhead(\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def hdfs_sampling(df, window='session'):\n",
    "    assert window == 'session', \"Only window=session is supported for HDFS dataset.\"\n",
    "    df['BlockId']= \"NULL\"\n",
    "    \n",
    "    data_dict = defaultdict(list) #preserve insertion order of items\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        blkId_list = re.findall(r'(blk_-?\\d+)', row['Content'])\n",
    "        blkId_set = set(blkId_list)\n",
    "        for blk_Id in blkId_set:\n",
    "            data_dict[blk_Id].append(row[\"Content\"])\n",
    "\n",
    "    data_df = pd.DataFrame(list(data_dict.items()), columns=['BlockId', 'EventSequence'])  \n",
    "    data_df.to_csv(log_sequence_file, index=None)\n",
    "    return data_df\n",
    "\n",
    "train= hdfs_sampling(train)\n",
    "train.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Sequence(hdfs_sequence_file, n=None, ratio=0.3):\n",
    "    blk_label_dict = {}\n",
    "    blk_label_file = os.path.join(input_dir, \"anomaly_label.csv\")\n",
    "    blk_df = pd.read_csv(blk_label_file)\n",
    "    for _ , row in tqdm(blk_df.iterrows()):\n",
    "        blk_label_dict[row[\"BlockId\"]] = 1 if row[\"Label\"] == \"Anomaly\" else 0\n",
    "\n",
    "    seq = pd.read_csv(hdfs_sequence_file)\n",
    "    seq[\"Label\"] = seq[\"BlockId\"].apply(lambda x: blk_label_dict.get(x)) #add label to the sequence of each blockid\n",
    "\n",
    "    #normal_seq = seq[seq[\"Label\"] == 0][\"EventSequence\"]\n",
    "    #normal_seq = normal_seq.sample(frac=1, random_state=20) # shuffle normal data\n",
    "\n",
    "    #abnormal_seq = seq[seq[\"Label\"] == 1][\"EventSequence\"]\n",
    "    #normal_len, abnormal_len = len(normal_seq), len(abnormal_seq)\n",
    "    #train_len = n if n else int(normal_len * ratio)\n",
    "    #print(\"normal size {0}, abnormal size {1}, training size {2}\".format(normal_len, abnormal_len, train_len))\n",
    "\n",
    "    #train = seq.iloc[:train_len]\n",
    "    #test_normal = normal_seq.iloc[train_len:]\n",
    "    #test_abnormal = abnormal_seq\n",
    "\n",
    "    #df_to_file(train, output_dir + \"train\")\n",
    "    #df_to_file(test_normal, output_dir + \"test_normal\")\n",
    "    #df_to_file(test_abnormal, output_dir + \"test_abnormal\")\n",
    "    print(\"generate train test data done\")\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "575061it [00:17, 32305.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate train test data done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockId</th>\n",
       "      <th>EventSequence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>['Receiving block blk_-1608999687919862906 src...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>['Receiving block blk_7503483334202473044 src:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>['Receiving block blk_-3544583377289625738 src...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-9073992586687739851</td>\n",
       "      <td>['Receiving block blk_-9073992586687739851 src...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_7854771516489510256</td>\n",
       "      <td>['Receiving block blk_7854771516489510256 src:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    BlockId  \\\n",
       "0  blk_-1608999687919862906   \n",
       "1   blk_7503483334202473044   \n",
       "2  blk_-3544583377289625738   \n",
       "3  blk_-9073992586687739851   \n",
       "4   blk_7854771516489510256   \n",
       "\n",
       "                                       EventSequence  Label  \n",
       "0  ['Receiving block blk_-1608999687919862906 src...      0  \n",
       "1  ['Receiving block blk_7503483334202473044 src:...      0  \n",
       "2  ['Receiving block blk_-3544583377289625738 src...      1  \n",
       "3  ['Receiving block blk_-9073992586687739851 src...      0  \n",
       "4  ['Receiving block blk_7854771516489510256 src:...      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = generate_Sequence(log_sequence_file)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the maximum number of words in a comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1791"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['EventSequence'].apply(lambda x:len(str(x).split())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing a function for getting auc score for validation\n",
    "\n",
    "def roc_auc(predictions,target):\n",
    "    '''\n",
    "    This methods returns the AUC Score when given the Predictions\n",
    "    and Labels\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.EventSequence.values, train.Label.values, \n",
    "                                                  stratify=train.Label.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 10\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "#zero pad the sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "xtrain_pad = pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 10)            4269780   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 10)                210       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,270,001\n",
      "Trainable params: 4,270,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: total: 328 ms\n",
      "Wall time: 157 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # A simpleRNN without any pretrained embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     10,\n",
    "                     input_length=max_len))\n",
    "    model.add(SimpleRNN(10))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2347/2347 [==============================] - 89s 37ms/step - loss: 0.1085 - accuracy: 0.9753\n",
      "Epoch 2/5\n",
      "2347/2347 [==============================] - 86s 37ms/step - loss: 0.0530 - accuracy: 0.9864\n",
      "Epoch 3/5\n",
      "2347/2347 [==============================] - 86s 37ms/step - loss: 0.0252 - accuracy: 0.9933\n",
      "Epoch 4/5\n",
      "2269/2347 [============================>.] - ETA: 2s - loss: 0.0046 - accuracy: 0.9987"
     ]
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64*strategy.num_replicas_in_sync) #Multiplying by Strategy to run on TPU's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model = []\n",
    "scores_model.append({'Model': 'SimpleRNN','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_seq[:1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4a2be7d9a8e225bb2e573ddefdac0c561e4de6358d3dd5bc426a91fd95e2205"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
