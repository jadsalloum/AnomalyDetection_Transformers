{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\miniconda\\lib\\site-packages (4.20.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\miniconda\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in c:\\miniconda\\lib\\site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\miniconda\\lib\\site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\miniconda\\lib\\site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\miniconda\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\miniconda\\lib\\site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: requests in c:\\miniconda\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\miniconda\\lib\\site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\miniconda\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\miniconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\miniconda\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: colorama in c:\\miniconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\miniconda\\lib\\site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\miniconda\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\miniconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\miniconda\\lib\\site-packages (from requests->transformers) (2021.10.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Miniconda\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blockid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>['Receiving block blk_-1608999687919862906 src...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_7503483334202473044</td>\n",
       "      <td>['Receiving block blk_7503483334202473044 src:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-3544583377289625738</td>\n",
       "      <td>['Receiving block blk_-3544583377289625738 src...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    blockid  \\\n",
       "1  blk_-1608999687919862906   \n",
       "2   blk_7503483334202473044   \n",
       "3  blk_-3544583377289625738   \n",
       "\n",
       "                                            sentence  label  \n",
       "1  ['Receiving block blk_-1608999687919862906 src...      0  \n",
       "2  ['Receiving block blk_7503483334202473044 src:...      0  \n",
       "3  ['Receiving block blk_-3544583377289625738 src...      1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# because the dataset is int tsv format we have to use delimeter.\n",
    "df = pd.read_csv(\"../output/hdfs/HDFS.log_labeled.csv.zip\" , names=[ 'blockid','sentence','label'])\n",
    "df = df.iloc[1:4 , :] # drop first row\n",
    "# creating a copy so we don't messed up our original dataset.\n",
    "data=df.copy()\n",
    "data['label']=data['label'].astype(float)\n",
    "data['label']=data['label'].astype(int)\n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Receiving block blk_-1608999687919862906 src...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Receiving block blk_7503483334202473044 src:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Receiving block blk_-3544583377289625738 src...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "1  ['Receiving block blk_-1608999687919862906 src...      0\n",
       "2  ['Receiving block blk_7503483334202473044 src:...      0\n",
       "3  ['Receiving block blk_-3544583377289625738 src...      1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(['blockid'],axis=1,inplace=True)\n",
    "sentences=data.sentence.values\n",
    "labels = data.label.values\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "# using the low level BERT for our task.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Printing the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Printing the tokenized sentence in form of list.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11928 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "for sent in sentences:\n",
    "    # so basically encode tokenizing , mapping sentences to thier token ids after adding special tokens.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence which are encoding.\n",
    "                        add_special_tokens = True, # Adding special tokens '[CLS]' and '[SEP]'\n",
    "\n",
    "                         )\n",
    "    \n",
    " \n",
    "    input_ids.append(encoded_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN , truncating=\"post\", padding=\"post\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Generating attention mask for sentences.\n",
    "    #   - when there is 0 present as token id we are going to set mask as 0.\n",
    "    #   - we are going to set mask 1 for all non-zero positive input id.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "   \n",
    "    attention_masks.append(att_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(validation_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torchvision as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch\n",
    "!pip uninstall torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'pytorch'\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Miniconda\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#conda install pytorch torchvision cudatoolkit=10.1 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jads\\AppData\\Local\\Temp\\2\\ipykernel_9528\\1545515418.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_inputs = torch.tensor(train_inputs)\n",
      "C:\\Users\\jads\\AppData\\Local\\Temp\\2\\ipykernel_9528\\1545515418.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_inputs = torch.tensor(validation_inputs)\n",
      "C:\\Users\\jads\\AppData\\Local\\Temp\\2\\ipykernel_9528\\1545515418.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(train_labels)\n",
      "C:\\Users\\jads\\AppData\\Local\\Temp\\2\\ipykernel_9528\\1545515418.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_labels = torch.tensor(validation_labels)\n",
      "C:\\Users\\jads\\AppData\\Local\\Temp\\2\\ipykernel_9528\\1545515418.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_masks = torch.tensor(train_masks)\n",
      "C:\\Users\\jads\\AppData\\Local\\Temp\\2\\ipykernel_9528\\1545515418.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_masks = torch.tensor(validation_masks)\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch\n",
    "#!pip install torchvision \n",
    "#import torch\n",
    "#import torchvision\n",
    "#changing the numpy arrays into tensors for working on GPU. \n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Deciding the batch size for training.\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# DataLoader for our validation(test) set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\jads\\AnomalyDetection_Transformers\\KaggleTest\\binarybert.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/binarybert.ipynb#ch0000009?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m BertForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/binarybert.ipynb#ch0000009?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/binarybert.ipynb#ch0000009?line=6'>7</a>\u001b[0m     num_labels \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m,   \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/binarybert.ipynb#ch0000009?line=7'>8</a>\u001b[0m     output_attentions \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/binarybert.ipynb#ch0000009?line=8'>9</a>\u001b[0m     output_hidden_states \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/binarybert.ipynb#ch0000009?line=9'>10</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/binarybert.ipynb#ch0000009?line=11'>12</a>\u001b[0m \u001b[39m# Running the model on GPU.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/jads/AnomalyDetection_Transformers/KaggleTest/binarybert.ipynb#ch0000009?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39;49mcuda()\n",
      "File \u001b[1;32mc:\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:689\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=671'>672</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=672'>673</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=673'>674</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=674'>675</a>\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=686'>687</a>\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=687'>688</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=688'>689</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[1;32mc:\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=578'>579</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=583'>584</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=589'>590</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=578'>579</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=583'>584</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=589'>590</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=578'>579</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=583'>584</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=589'>590</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=597'>598</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=598'>599</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=599'>600</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=600'>601</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=601'>602</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=602'>603</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=603'>604</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:689\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=671'>672</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=672'>673</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=673'>674</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=674'>675</a>\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=686'>687</a>\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=687'>688</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/miniconda/lib/site-packages/torch/nn/modules/module.py?line=688'>689</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[1;32mc:\\miniconda\\lib\\site-packages\\torch\\cuda\\__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/cuda/__init__.py?line=206'>207</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/cuda/__init__.py?line=207'>208</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/cuda/__init__.py?line=208'>209</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/cuda/__init__.py?line=209'>210</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> <a href='file:///c%3A/miniconda/lib/site-packages/torch/cuda/__init__.py?line=210'>211</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/cuda/__init__.py?line=211'>212</a>\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/cuda/__init__.py?line=212'>213</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/miniconda/lib/site-packages/torch/cuda/__init__.py?line=213'>214</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels = 2,   \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "# Running the model on GPU.\n",
    "model.cuda()\n",
    "\n",
    "# Running the model on CPU.\n",
    "#model.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "    # putting model in traing mode there are two model eval and train for model\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        #checking time taken after every 50 steps.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        #getting ids,mask,labes for every batch\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        \n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # doing back propagation\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "182fe3b26f900d191053996287a94e64fbebd9ca1b4c644d38d8e1b9f7b6df61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
